{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# v3.classification\n",
    "# 3/12/2018\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import sklearn.feature_extraction.image\n",
    "from torchvision.models import DenseNet\n",
    "import tables\n",
    "#-----helper function to split data into batches\n",
    "\n",
    "\n",
    "# ----- parse command line arguments\n",
    "# parser = argparse.ArgumentParser(description='Make output for entire image using Unet')\n",
    "# parser.add_argument('input_pattern',\n",
    "#                     help=\"input filename pattern. try: *.png, or tsv file containing list of files to analyze\",\n",
    "#                     nargs=\"*\")\n",
    "\n",
    "# parser.add_argument('-p', '--patchsize', help=\"patchsize, default 256\", default=256, type=int)\n",
    "# parser.add_argument('-s', '--batchsize', help=\"batchsize for controlling GPU memory usage, default 10\", default=10, type=int)\n",
    "# parser.add_argument('-o', '--outdir', help=\"outputdir, default ./output/\", default=\"./output/\", type=str)\n",
    "# parser.add_argument('-r', '--resize', help=\"resize factor 1=1x, 2=2x, .5 = .5x\", default=1, type=float)\n",
    "# parser.add_argument('-m', '--model', help=\"model\", default=\"best_model.pth\", type=str)\n",
    "# parser.add_argument('-i', '--gpuid', help=\"id of gpu to use\", default=0, type=int)\n",
    "# parser.add_argument('-f', '--force', help=\"force regeneration of output even if it exists\", default=False,\n",
    "#                     action=\"store_true\")\n",
    "# parser.add_argument('-b', '--basepath',\n",
    "#                     help=\"base path to add to file names, helps when producing data using tsv file as input\",\n",
    "#                     default=\"\", type=str)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# #args\n",
    "\n",
    "# if not (args.input_pattern):\n",
    "#     parser.error('No images selected with input pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_batch(l,hc, n): \n",
    "    for i in range(0, l.shape[0], n):\n",
    "        x = l[i:i + n,:]\n",
    "        y = hc[i:i + n,:]\n",
    "        yield x , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'E:\\\\MvP\\\\FinalExperimentWithPancreas\\\\NewRNG\\\\Run3\\\\DeepLearningHeatmap\\\\OrganSiteClassifying\\\\FusionNew\\\\Test1Out\\\\'\n",
    "resize = 1\n",
    "# batch_size = args.batchsize\n",
    "batch_size = 100\n",
    "patch_size = 376\n",
    "stride_size = 376//2\n",
    "base_path = 'E:\\\\MvP\\\\FinalExperimentWithPancreas\\\\NewRNG\\\\Run3\\\\DeepLearningHeatmap\\\\OrganSiteClassifying\\\\Test1\\\\**\\\\'\n",
    "input_pattern = '*.png'\n",
    "mdlPath = 'CUPBinlinFusion_densenet_best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# increasing few layers in our model\n",
    "class DenseNetFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNetFusion, self).__init__()\n",
    "        self.image_dense_layer_1 = nn.Linear(300 , 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm1 = nn.BatchNorm1d(100)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.image_dense_layer_2 = nn.Linear(100, 10)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.BilinearPool = nn.Bilinear(10, 10, 10)\n",
    "        self.final = nn.Linear(10 , 4)\n",
    "        self.tabular_dense_layer_1 = nn.Linear(10, 10)\n",
    "#         self.tabular_dense_layer_2 = nn.Linear(8, 5)        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self, image_inputs , tabular_data_inputs):\n",
    "#       DL data\n",
    "        x_img = image_inputs\n",
    "        x_img = self.image_dense_layer_1(x_img)\n",
    "        x_img = self.relu(x_img)\n",
    "        x_img = self.batchnorm1(x_img)\n",
    "        x_img = self.dropout(x_img)\n",
    "        x_img = self.image_dense_layer_2(x_img)\n",
    "        x_img = self.relu(x_img)\n",
    "\n",
    "        \n",
    "#       Tabular data\n",
    "        tab = self.tabular_dense_layer_1(torch.tensor(tabular_data_inputs, device=device).float())\n",
    "        tab = self.relu(tab)\n",
    "        x = self.BilinearPool(x_img,tab)\n",
    "#         x = torch.cat((x, tab), dim=1)\n",
    "\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \t32474\n"
     ]
    }
   ],
   "source": [
    "# ----- load network\n",
    "device = torch.device(0 if 0!=-2 and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = torch.load(mdlPath, map_location=lambda storage, loc: storage) #load checkpoint to CPU and then put to device https://discuss.pytorch.org/t/saving-and-loading-torch-models-on-2-machines-with-different-number-of-gpu-devices/6666\n",
    "model = DenseNetFusion()\n",
    "model = model.to(device)\n",
    "# model = DenseNet(growth_rate=checkpoint[\"growth_rate\"], block_config=checkpoint[\"block_config\"],\n",
    "#                  num_init_features=checkpoint[\"num_init_features\"], bn_size=checkpoint[\"bn_size\"],\n",
    "#                  drop_rate=checkpoint[\"drop_rate\"], num_classes=checkpoint[\"num_classes\"]).to(device)\n",
    "# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# model.cuda(0)\n",
    "\n",
    "\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")\n",
    "\n",
    "# ----- get file list\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "files = []\n",
    "# basepath = args.basepath  #\n",
    "# basepath = basepath + os.sep if len(\n",
    "#     basepath) > 0 else \"\"  # if the user supplied a different basepath, make sure it ends with an os.sep\n",
    "\n",
    "# if len(args.input_pattern) > 1:  # bash has sent us a list of files\n",
    "#     files = args.input_pattern\n",
    "# elif args.input_pattern[0].endswith(\"tsv\"):  # user sent us an input file\n",
    "#     # load first column here and store into files\n",
    "#     print('loading tsv file...')\n",
    "#     with open(args.input_pattern[0], 'r') as f: #added encoding=\"ISO-8859-1\" to fix UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd0 in position 0: invalid continuation byte\n",
    "#         for line in f:\n",
    "#             print(line)\n",
    "#             if line[0] == \"#\":\n",
    "#                 continue\n",
    "#                 print('Continue')\n",
    "#             files.append(basepath + line.strip().split(\"\\t\")[0])\n",
    "# else:  # user sent us a wildcard, need to use glob to find files\n",
    "#     files = glob.glob(args.basepath + args.input_pattern[0])\n",
    "files = glob.glob(base_path + input_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "testTable = tables.open_file('CUPBinlinFusion_Test.pytable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'79M_100353_12289_43'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_path + input_pattern\n",
    "# files = [ 'E:\\\\MvP\\\\FinalExperimentWithPancreas\\\\NewRNG\\\\Run3\\\\DeepLearningHeatmap\\\\Val\\\\Pri\\\\40P (2)_100097_12033.png','E:\\\\MvP\\\\FinalExperimentWithPancreas\\\\NewRNG\\\\Run3\\\\DeepLearningHeatmap\\\\Val\\\\Pri\\\\40P (2)_100097_13569.png',\n",
    "#  'E:\\\\MvP\\\\FinalExperimentWithPancreas\\\\NewRNG\\\\Run3\\\\DeepLearningHeatmap\\\\Val\\\\Pri\\\\40P (2)_100097_13825.png']\n",
    "os.path.basename(testTable.root.filenames[0].strip())[0:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNetFusion(\n",
       "  (image_dense_layer_1): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (batchnorm1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
       "  (image_dense_layer_2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (BilinearPool): Bilinear(in1_features=10, in2_features=10, out_features=10, bias=True)\n",
       "  (final): Linear(in_features=10, out_features=4, bias=True)\n",
       "  (tabular_dense_layer_1): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14502, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTable.root.DLfeats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/ (RootGroup) ''\n",
       "  children := ['DLfeats' (EArray), 'HCFeats' (EArray), 'classsizes' (CArray), 'filenames' (EArray), 'imgs' (EArray), 'labels' (EArray)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTable.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\mvp\\venv\\lib\\site-packages\\ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de273f683ee3453aa85d86d36c093041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14502.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\mvp\\venv\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# +\n",
    "# ------ work on files\n",
    "#print(files)\n",
    "saveScore = []\n",
    "from tqdm import tqdm_notebook\n",
    "classPred= open(OUTPUT_DIR+\"\\classPred.txt\",\"w+\")\n",
    "classPredScore= open(OUTPUT_DIR+\"\\classPredScore.txt\",\"w+\")\n",
    "classScore = open(OUTPUT_DIR+\"\\classScore.txt\",\"w+\")\n",
    "classPredDetailed = open(OUTPUT_DIR+\"\\classPredDetailed.txt\",\"w+\")\n",
    "for i in tqdm_notebook(range(testTable.root.imgs.shape[0])):\n",
    "    \n",
    "    fname = testTable.root.filenames[i].strip()\n",
    "    \n",
    "    newfname_class = \"%s/%s_class.png\" % (OUTPUT_DIR, os.path.basename(fname)[0:-4])\n",
    "\n",
    "#     print(f\"working on file: \\t {fname}\")\n",
    "\n",
    "    # if not args.force and os.path.exists(newfname_class):\n",
    "    #     print(\"Skipping as output file exists\")\n",
    "    #     continue\n",
    "    #\n",
    "    # cv2.imwrite(newfname_class, np.zeros(shape=(1, 1)))\n",
    "\n",
    "    \n",
    "#     io = cv2.cvtColor(cv2.imread(fname),cv2.COLOR_BGR2RGB)\n",
    "#     io = cv2.resize(io, (0, 0), fx=1, fy=1)\n",
    "\n",
    "#     io_shape_orig = np.array(io.shape)\n",
    "    \n",
    "#     #add half the stride as padding around the image, so that we can crop it away later\n",
    "#     io = np.pad(io, [(stride_size//2, stride_size//2), (stride_size//2, stride_size//2), (0, 0)], mode=\"reflect\")\n",
    "    \n",
    "#     io_shape_wpad = np.array(io.shape)\n",
    "    \n",
    "#     #pad to match an exact multiple of unet patch size, otherwise last row/column are lost\n",
    "#     npad0 = int(np.ceil(io_shape_wpad[0] / patch_size) * patch_size - io_shape_wpad[0])\n",
    "#     npad1 = int(np.ceil(io_shape_wpad[1] / patch_size) * patch_size - io_shape_wpad[1])\n",
    "\n",
    "#     io = np.pad(io, [(0, npad0), (0, npad1), (0, 0)], mode=\"constant\")\n",
    "\n",
    "#     arr_out = sklearn.feature_extraction.image.extract_patches(io,(patch_size,patch_size,3),stride_size)\n",
    "    DL = torch.tensor(testTable.root.DLfeats[i,:].reshape(1,300)).to(device)\n",
    "    HC = torch.tensor(testTable.root.HCFeats[i,:].reshape(1,10)).to(device)\n",
    "#     arr_out = sklearn.feature_extraction.image.extract_patches(io,(patch_size,patch_size),stride_size)\n",
    "#     arr_out_shape = arr_out.shape\n",
    "#     arr_out = arr_out.reshape(-1,patch_size,patch_size,1)\n",
    "\n",
    "    #in case we have a large network, lets cut the list of tiles into batches\n",
    "    output = np.zeros((0,checkpoint[\"num_classes\"]))\n",
    "    output_batch = model(DL,HC)\n",
    "    output = np.append(output,output_batch.detach().cpu().numpy(),axis=0)\n",
    "    \n",
    "#     for dl_arr,hc_arr in divide_batch(DL,HC,batch_size):\n",
    "        \n",
    "        \n",
    "#         hc = torch.from_numpy(hc_arr).type('torch.FloatTensor').to(device)\n",
    "\n",
    "#         # ---- get results\n",
    "        \n",
    "\n",
    "#         # --- pull from GPU and append to rest of output \n",
    "#         output_batch = output_batch.detach().cpu().numpy()\n",
    "        \n",
    "\n",
    "\n",
    "#     print(output.shape)\n",
    "    saveScore.append(output)\n",
    "#     tileclass = np.argmax(output, axis=1)\n",
    "#     print(output)\n",
    "#     print(tileclass)\n",
    "#     asdab\n",
    "#     predc,predccounts=np.unique(tileclass, return_counts=True)\n",
    "#     print(f'predcounts{predccounts}')\n",
    "# #     if len(predccounts)==1:\n",
    "# #         score = 0\n",
    "# #     else:\n",
    "# #         score = predccounts[1]/sum(predccounts)\n",
    "# #     classPredDetailed.write(f\"working on file: \\t {fname}\\n\")\n",
    "    \n",
    "# #     print(f\" score: \\t{score}\")\n",
    "#     for c,cc in zip(predc,predccounts):\n",
    "#         score = cc/(output.shape[0])\n",
    "#         classPredDetailed.write(f\"class/count: \\t{c}\\t{cc}\\n\")\n",
    "#         print(f\"class/count: \\t{c}\\t{cc}\")\n",
    "#         print(f\"class {c} score: \\t{score}\")\n",
    "        \n",
    "\n",
    "\n",
    "#     classScore.write(f\"{score}\\n\")\n",
    "#     print(f\"predicted class argmax([cc0,cc1]):\\t{predc[np.argmax(predccounts)]}\")\n",
    "#     classPred.write(f\"{predc[np.argmax(predccounts)]}\\n\")\n",
    "# #     classPredDetailed.write(\"\\n\")\n",
    "#     print(f\"predicted class score>0.5:\\t{(score>0.5)*1}\")\n",
    "#     classPredScore.write(f\"{(score>0.5)*1}\\n\")\n",
    "    \n",
    "classPred.close()\n",
    "# classPredDetailed.close()\n",
    "classScore.close()\n",
    "classPredScore.close()\n",
    "ss = np.array(saveScore)\n",
    "np.savez(OUTPUT_DIR+\"classPred.npz\", x=ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\MvP\\\\FinalExperimentWithPancreas\\\\NewRNG\\\\Run3\\\\DeepLearningHeatmap\\\\OrganSiteClassifying\\\\FusionNew\\\\Test1Out\\\\classPred.npz'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR+\"classPred.npz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14502, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTable.root.DLfeats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.savez(\"classPred032322.npz\", x=ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR+\"classPred.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR+\"classPred.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tileclass = np.argmax(output, axis=1)\n",
    "predc,predccounts=np.unique(tileclass, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tileclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss = np.setdiff1d([0, 1, 2, 3], predc)\n",
    "np.where([0, 1, 2, 3]== miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predccounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,cc in zip(predc,predccounts):\n",
    "    score = cc/(output.shape[0])\n",
    "    classPredDetailed.write(f\"class/count: \\t{c}\\t{cc}\\n\")\n",
    "    print(f\"class/count: \\t{c}\\t{cc}\")\n",
    "    print(f\"class {c} score: \\t{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- load network\n",
    "device = torch.device(0 if 0!=-2 and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = torch.load(mdlPath, map_location=lambda storage, loc: storage) #load checkpoint to CPU and then put to device https://discuss.pytorch.org/t/saving-and-loading-torch-models-on-2-machines-with-different-number-of-gpu-devices/6666\n",
    "\n",
    "model = DenseNet(growth_rate=checkpoint[\"growth_rate\"], block_config=checkpoint[\"block_config\"],\n",
    "                 num_init_features=checkpoint[\"num_init_features\"], bn_size=checkpoint[\"bn_size\"],\n",
    "                 drop_rate=checkpoint[\"drop_rate\"], num_classes=checkpoint[\"num_classes\"]).to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")\n",
    "\n",
    "# ----- get file list\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "files = []\n",
    "# basepath = args.basepath  #\n",
    "# basepath = basepath + os.sep if len(\n",
    "#     basepath) > 0 else \"\"  # if the user supplied a different basepath, make sure it ends with an os.sep\n",
    "\n",
    "# if len(args.input_pattern) > 1:  # bash has sent us a list of files\n",
    "#     files = args.input_pattern\n",
    "# elif args.input_pattern[0].endswith(\"tsv\"):  # user sent us an input file\n",
    "#     # load first column here and store into files\n",
    "#     print('loading tsv file...')\n",
    "#     with open(args.input_pattern[0], 'r') as f: #added encoding=\"ISO-8859-1\" to fix UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd0 in position 0: invalid continuation byte\n",
    "#         for line in f:\n",
    "#             print(line)\n",
    "#             if line[0] == \"#\":\n",
    "#                 continue\n",
    "#                 print('Continue')\n",
    "#             files.append(basepath + line.strip().split(\"\\t\")[0])\n",
    "# else:  # user sent us a wildcard, need to use glob to find files\n",
    "#     files = glob.glob(args.basepath + args.input_pattern[0])\n",
    "files = glob.glob(base_path + input_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path + input_pattern\n",
    "# files = [ 'E:\\\\MvP\\\\FinalExperimentWithPancreas\\\\NewRNG\\\\Run3\\\\DeepLearningHeatmap\\\\Val\\\\Pri\\\\40P (2)_100097_12033.png','E:\\\\MvP\\\\FinalExperimentWithPancreas\\\\NewRNG\\\\Run3\\\\DeepLearningHeatmap\\\\Val\\\\Pri\\\\40P (2)_100097_13569.png',\n",
    "#  'E:\\\\MvP\\\\FinalExperimentWithPancreas\\\\NewRNG\\\\Run3\\\\DeepLearningHeatmap\\\\Val\\\\Pri\\\\40P (2)_100097_13825.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +\n",
    "# ------ work on files\n",
    "#print(files)\n",
    "classPred= open(OUTPUT_DIR+\"\\classPred.txt\",\"w+\")\n",
    "classPredScore= open(OUTPUT_DIR+\"\\classPredScore.txt\",\"w+\")\n",
    "classScore = open(OUTPUT_DIR+\"\\classScore.txt\",\"w+\")\n",
    "# classPredDetailed = open(OUTPUT_DIR+\"\\classPredDetailed.txt\",\"w+\")\n",
    "for fname in files:\n",
    "\n",
    "    fname = fname.strip()\n",
    "    newfname_class = \"%s/%s_class.png\" % (OUTPUT_DIR, os.path.basename(fname)[0:-4])\n",
    "\n",
    "    print(f\"working on file: \\t {fname}\")\n",
    "\n",
    "    # if not args.force and os.path.exists(newfname_class):\n",
    "    #     print(\"Skipping as output file exists\")\n",
    "    #     continue\n",
    "    #\n",
    "    # cv2.imwrite(newfname_class, np.zeros(shape=(1, 1)))\n",
    "\n",
    "    \n",
    "    io = cv2.cvtColor(cv2.imread(fname),cv2.COLOR_BGR2RGB)\n",
    "    io = cv2.resize(io, (0, 0), fx=1, fy=1)\n",
    "\n",
    "    io_shape_orig = np.array(io.shape)\n",
    "    \n",
    "    #add half the stride as padding around the image, so that we can crop it away later\n",
    "    io = np.pad(io, [(stride_size//2, stride_size//2), (stride_size//2, stride_size//2), (0, 0)], mode=\"reflect\")\n",
    "    \n",
    "    io_shape_wpad = np.array(io.shape)\n",
    "    \n",
    "    #pad to match an exact multiple of unet patch size, otherwise last row/column are lost\n",
    "    npad0 = int(np.ceil(io_shape_wpad[0] / patch_size) * patch_size - io_shape_wpad[0])\n",
    "    npad1 = int(np.ceil(io_shape_wpad[1] / patch_size) * patch_size - io_shape_wpad[1])\n",
    "\n",
    "    io = np.pad(io, [(0, npad0), (0, npad1), (0, 0)], mode=\"constant\")\n",
    "\n",
    "    arr_out = sklearn.feature_extraction.image.extract_patches(io,(patch_size,patch_size,3),stride_size)\n",
    "    arr_out_shape = arr_out.shape\n",
    "    arr_out = arr_out.reshape(-1,patch_size,patch_size,3)\n",
    "\n",
    "    #in case we have a large network, lets cut the list of tiles into batches\n",
    "    output = np.zeros((0,checkpoint[\"num_classes\"]))\n",
    "    \n",
    "    for batch_arr in divide_batch(arr_out,batch_size):\n",
    "        \n",
    "        arr_out_gpu = torch.from_numpy(batch_arr.transpose(0, 3, 1, 2) / 255).type('torch.FloatTensor').to(device)\n",
    "\n",
    "        # ---- get results\n",
    "        output_batch = model(arr_out_gpu)\n",
    "\n",
    "        # --- pull from GPU and append to rest of output \n",
    "        output_batch = output_batch.detach().cpu().numpy()\n",
    "        output = np.append(output,output_batch,axis=0)\n",
    "\n",
    "\n",
    "    print(output.shape)\n",
    "    tileclass = np.argmax(output, axis=1)\n",
    "    predc,predccounts=np.unique(tileclass, return_counts=True)\n",
    "    print(f'predcounts{predccounts}')\n",
    "    if len(predccounts)==1:\n",
    "        score = 0\n",
    "    else:\n",
    "        score = predccounts[1]/sum(predccounts)\n",
    "#     classPredDetailed.write(f\"working on file: \\t {fname}\\n\")\n",
    "    \n",
    "    print(f\" score: \\t{score}\")\n",
    "#     for c,cc in zip(predc,predccounts):\n",
    "#         score = cc/(output.shape[0])\n",
    "#         classPredDetailed.write(f\"class/count: \\t{c}\\t{cc}\\n\")\n",
    "#         print(f\"class/count: \\t{c}\\t{cc}\")\n",
    "#         print(f\"class {c} score: \\t{score}\")\n",
    "        \n",
    "\n",
    "\n",
    "    classScore.write(f\"{score}\\n\")\n",
    "    print(f\"predicted class argmax([cc0,cc1]):\\t{predc[np.argmax(predccounts)]}\")\n",
    "    classPred.write(f\"{predc[np.argmax(predccounts)]}\\n\")\n",
    "#     classPredDetailed.write(\"\\n\")\n",
    "    print(f\"predicted class score>0.5:\\t{(score>0.5)*1}\")\n",
    "    classPredScore.write(f\"{(score>0.5)*1}\\n\")\n",
    "    \n",
    "classPred.close()\n",
    "# classPredDetailed.close()\n",
    "classScore.close()\n",
    "classPredScore.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
